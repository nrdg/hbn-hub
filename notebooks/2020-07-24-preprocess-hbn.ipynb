{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richford/miniconda3/envs/cloudknot_qsiprep/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/Users/richford/miniconda3/envs/cloudknot_qsiprep/lib/python3.7/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "/Users/richford/miniconda3/envs/cloudknot_qsiprep/lib/python3.7/site-packages/dipy/stats/__init__.py:7: UserWarning: The `dipy.stats` module is still under heavy development and functionality, as well as the API is likely to change in future versions of the software\n",
      "  warnings.warn(w_string)\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "import AFQ.data as afqd\n",
    "import cloudknot as ck\n",
    "import importlib\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cloudknot' from '/Users/richford/projects/cloudknot/cloudknot/cloudknot/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richford/miniconda3/envs/cloudknot_qsiprep/lib/python3.7/site-packages/bids/layout/models.py:102: FutureWarning: The 'extension' entity currently excludes the leading dot ('.'). As of version 0.14.0, it will include the leading dot. To suppress this warning and include the leading dot, use `bids.config.set_option('extension_initial_dot', True)`.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving subject S3 keys\n",
      "[########################################] | 100% Completed |  5min 19.9s\n"
     ]
    }
   ],
   "source": [
    "hbn = afqd.HBNSite(site=\"Site-RU\", subjects=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-west-2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ck.get_region()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck.set_region('us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "fs.touch(\"hbn-derivatives/qsiprep-ck/useless_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_hbn_ru_subject(subject_id):\n",
    "    import AFQ.data as afqd\n",
    "    import os\n",
    "    import subprocess\n",
    "    import sys\n",
    "    from bids import BIDSLayout\n",
    "    from s3fs import S3FileSystem\n",
    "        \n",
    "    hbn_ru = afqd.HBNSite(site=\"Site-RU\", subjects=subject_id)\n",
    "    hbn_ru.download(\"./hbn-ru\")\n",
    "    fs = S3FileSystem()\n",
    "    \n",
    "    layout = BIDSLayout(\"./hbn-ru\")\n",
    "    dwi_subs = layout.get(return_type=\"id\", target=\"subject\", datatype=\"dwi\")\n",
    "    dwi_subs = [\"sub-\" + sub for sub in dwi_subs]\n",
    "    \n",
    "    # HBN has other files that we don't need for dMRI preproc and whose presence will break qsiprep\n",
    "    # Get rid of them\n",
    "    s0 = hbn_ru.subjects[0]\n",
    "    tracew_dwi_files = [file for key, file in s0.files[\"raw\"].items() if \"TRACEW\" in key]\n",
    "    fmri_files = [file for key, file in s0.files[\"raw\"].items() if \"/func/\" in key]\n",
    "    fmri_files += [file for key, file in s0.files[\"raw\"].items() if \"/fmap/\" in key and \"fMRI\" in key]\n",
    "    \n",
    "    for fname in fmri_files + tracew_dwi_files:\n",
    "        os.remove(fname)\n",
    "    \n",
    "    if subject_id in dwi_subs:\n",
    "        response = subprocess.run(\n",
    "            [\n",
    "                \"qsiprep\",\n",
    "                \"--output-resolution\",\n",
    "                \"1.8\",\n",
    "                \"--participant-label\",\n",
    "                subject_id,\n",
    "                \"-w\",\n",
    "                \"./hbn-wrk\",\n",
    "                \"--nthreads\",\n",
    "                \"8\",\n",
    "                \"--omp-nthreads\",\n",
    "                \"8\",\n",
    "                \"--force-spatial-normalization\",\n",
    "                \"--dwi-denoise-window\",\n",
    "                \"5\",\n",
    "                \"--combine-all-dwis\",\n",
    "                \"--unringing-method\",\n",
    "                \"mrdegibbs\",\n",
    "                \"./hbn-ru\",\n",
    "                \"./hbn-preproc\",\n",
    "                \"participant\",\n",
    "            ],\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        output_dir = \"hbn-derivatives/qsiprep-ck\"            \n",
    "        fs.put(f\"./hbn-preproc/qsiprep/{subject_id}\",\n",
    "               \"/\".join([output_dir, subject_id]), recursive=True)\n",
    "        fs.put(f\"./hbn-preproc/qsiprep/{subject_id}.html\",\n",
    "               \"/\".join([output_dir, subject_id + \".html\"]))\n",
    "\n",
    "        return {subject_id: True}\n",
    "    else:\n",
    "        return {subject_id: False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a cloudknot DockerImage instance\n",
    "\n",
    "where the base_image is a customized version of qsiprep\n",
    "This version of the qsiprep docker image is located on AWS ECR with image URI:\n",
    "[454929164628.dkr.ecr.us-east-1.amazonaws.com/qsiprep:nrdg](https://console.aws.amazon.com/ecr/repositories/qsiprep/?region=us-east-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cloudknot.dockerimage:Found docker-image preprocess-hbn-ru-subject in your config file but the input parameters have changed. The updated parameters are ['func', 'github_installs']. Continuing with the new input parameters and disregarding any old, potentially conflicting ones.\n",
      "WARNING:cloudknot.dockerimage:Warning, some imports not found by pipreqs. You will need to edit the Dockerfile by hand, e.g by installing from github. You need to install the following packages ['AFQ']\n"
     ]
    }
   ],
   "source": [
    "di = ck.DockerImage(\n",
    "    name=\"preprocess-hbn-ru-subject\",\n",
    "    func=preprocess_hbn_ru_subject,\n",
    "    base_image=\"qsiprep:nrdg\",\n",
    "    github_installs=[\"https://github.com/richford/pyAFQ.git@s3-bids-fetch\",\n",
    "                     \"https://github.com/bids-standard/pybids.git@0.9.3\"],\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, tag, and push the Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "di.build(tags=[\"hbn-ru-preproc-20200723\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = ck.aws.DockerRepo(name=ck.get_ecr_repo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'454929164628.dkr.ecr.us-east-1.amazonaws.com/cloudknot'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.repo_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The very first time you run this, this command could take\n",
    "# a few hours because the docker image is large\n",
    "di.push(repo=repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1227"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hbn.subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Knots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cloudknot.cloudknot:You specified configuration arguments for a knot that already exists. Cloudknot has returned the pre-existing knot, ignoring all of your other input parameters, which may or may not be the same. You should proceed with caution and confirm that this knot's parameters are as expected. If you want to be extra-safe, choose a different name or clobber this pre-existing knot and instantiate a new one with your input arguments.\n",
      "WARNING:cloudknot.dockerimage:Warning, some imports not found by pipreqs. You will need to edit the Dockerfile by hand, e.g by installing from github. You need to install the following packages ['AFQ']\n",
      "WARNING:cloudknot.cloudknot:You specified configuration arguments for a knot that already exists. Cloudknot has returned the pre-existing knot, ignoring all of your other input parameters, which may or may not be the same. You should proceed with caution and confirm that this knot's parameters are as expected. If you want to be extra-safe, choose a different name or clobber this pre-existing knot and instantiate a new one with your input arguments.\n",
      "WARNING:cloudknot.dockerimage:Warning, some imports not found by pipreqs. You will need to edit the Dockerfile by hand, e.g by installing from github. You need to install the following packages ['AFQ']\n",
      "WARNING:cloudknot.cloudknot:You specified configuration arguments for a knot that already exists. Cloudknot has returned the pre-existing knot, ignoring all of your other input parameters, which may or may not be the same. You should proceed with caution and confirm that this knot's parameters are as expected. If you want to be extra-safe, choose a different name or clobber this pre-existing knot and instantiate a new one with your input arguments.\n",
      "WARNING:cloudknot.dockerimage:Warning, some imports not found by pipreqs. You will need to edit the Dockerfile by hand, e.g by installing from github. You need to install the following packages ['AFQ']\n",
      "WARNING:cloudknot.cloudknot:You specified configuration arguments for a knot that already exists. Cloudknot has returned the pre-existing knot, ignoring all of your other input parameters, which may or may not be the same. You should proceed with caution and confirm that this knot's parameters are as expected. If you want to be extra-safe, choose a different name or clobber this pre-existing knot and instantiate a new one with your input arguments.\n",
      "WARNING:cloudknot.dockerimage:Warning, some imports not found by pipreqs. You will need to edit the Dockerfile by hand, e.g by installing from github. You need to install the following packages ['AFQ']\n"
     ]
    }
   ],
   "source": [
    "sub_ids = [s.subject_id for s in hbn.subjects]\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "chunk_knots = {\n",
    "    n: {\n",
    "        # Specify bid_percentage to use Spot instances\n",
    "        # And make sure the volume size is large enough. 50-55 GB seems about right for HBN preprocessing. YMMV.\n",
    "        \"knot\": ck.Knot(\n",
    "            name=f\"hbn_preproc_20200804_ch{n}\",\n",
    "            docker_image=di,\n",
    "            pars_policies=('AmazonS3FullAccess',),\n",
    "            bid_percentage=100,\n",
    "            memory=64000,\n",
    "            job_def_vcpus=8,\n",
    "            volume_size=55\n",
    "        ),\n",
    "        \"subjects\": chunk\n",
    "    } for n, chunk in enumerate(chunks(sub_ids, 320))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get subject IDs and predict output\n",
    "\n",
    "We'll pass these subject IDs to `knot.map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ids = [s.subject_id for s in hbn.subjects]\n",
    "sub_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some HBN subjects do not have dwi data. These will fail and return `{subject_id: False}`. Let's predict how many will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwi_predict = {\n",
    "    sub.subject_id: any([\"dwi\" in s3key for s3key in sub.s3_keys[\"raw\"]])\n",
    "    for sub in hbn.subjects\n",
    "}\n",
    "dwi_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the jobs and check on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in chunk_knots.keys():\n",
    "    chunk_knots[i] = {\n",
    "        \"knot\": chunk_knots[i][\"knot\"],\n",
    "        \"subjects\": chunk_knots[i][\"subjects\"],\n",
    "        \"results\": chunk_knots[i][\"knot\"].map(chunk_knots[i][\"subjects\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck.set_region('us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = knot.jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j0 = jobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j0.terminate(\"Output to S3 is undesirable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot.view_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([j.status for j in knot.jobs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The results are dicts where the keys are the subject IDs and the values report sucess or failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_futures.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out how many subjects we have in the entire study to support some cost estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites = {}\n",
    "\n",
    "for site in [\"Site-SI\", \"Site-CBIC\", \"Site-RU\", \"Site-CUNY\"]:\n",
    "    all_sites[site] = afqd.HBNSite(site=site)\n",
    "    print(f\"{site}: {len(all_sites[site]._all_subjects)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When you're done, clobber the knot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot.clobber(clobber_pars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
